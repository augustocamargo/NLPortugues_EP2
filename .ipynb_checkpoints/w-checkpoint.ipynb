{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0-rc3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo as variáveis do projeto\n",
    "vocab_size = 200000  # Considerar 200k palavras\n",
    "maxlen = 200  # Considerar apenas as 100 primeiras palavras do texto da review\n",
    "\n",
    "embed_dim = 50 # tamanho do Embedding de cada token ( também do word2vec da NILC)\n",
    "num_heads = 2  # N. de cabeças de atenção\n",
    "ff_dim = 32   # tamanho da camada oculta nas redes feed forward dentro do transformer\n",
    "\n",
    "# Path para o arquivo de dados da b2w\n",
    "B2W_DATAFILE = \"/home/wseidel/workspaces/usp/b2w-reviews01/B2W-Reviews01.csv\"\n",
    "# B2W_DATAFILE = \"/home/wseidel/workspaces/usp/b2w-reviews01/B2W-10k.csv\"\n",
    "\n",
    "# Path para o arquivo de dados de embeddings do NILC\n",
    "NILC_W2V_DATAFILE = \"/home/wseidel/workspaces/usp/NILC/word2vec_200k.txt\"\n",
    "\n",
    "# Quantidade de epocas para o treino\n",
    "QNT_EPOCAS_A_TREINAR = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados a serem analisados\n",
    "b2wCorpus = pd.read_csv(B2W_DATAFILE, sep=';', usecols=[\"review_text\", \"overall_rating\"])\n",
    "\n",
    "# Carregar o Word2Vec do NILC\n",
    "model_w2v = KeyedVectors.load_word2vec_format(NILC_W2V_DATAFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overall_rating</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>47955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_text\n",
       "overall_rating             \n",
       "1                     27369\n",
       "2                      8389\n",
       "3                     16315\n",
       "4                     32345\n",
       "5                     47955"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2wCorpus.groupby(['overall_rating']).count()\n",
    "# b2wCorpus.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train..: 79423 0.6\n",
      "test...: 39712 0.3\n",
      "val....: 13238 0.1\n",
      "----------------------------------------\n",
      "x_train..: 50\n",
      "x_test...: 50\n",
      "x_val....: 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_text_clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overall_rating</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27369</td>\n",
       "      <td>27369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8389</td>\n",
       "      <td>8389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16315</td>\n",
       "      <td>16315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32345</td>\n",
       "      <td>32345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47955</td>\n",
       "      <td>47955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_text  review_text_clean\n",
       "overall_rating                                \n",
       "0                     27369              27369\n",
       "1                      8389               8389\n",
       "2                     16315              16315\n",
       "3                     32345              32345\n",
       "4                     47955              47955"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_test_val_split(dataset, train_size=0.6, test_size=0.3, colname_stratify='overall_rating',random_seed=29):\n",
    "    val_size = 1 - round((train_size + test_size),1)\n",
    "    split_train_test_size = test_size + val_size\n",
    "\n",
    "    train, val = train_test_split(dataset, \n",
    "                                  test_size=split_train_test_size, \n",
    "                                  stratify=dataset[colname_stratify], \n",
    "                                  random_state=random_seed)\n",
    "\n",
    "    test, val = train_test_split(val, \n",
    "                                  test_size=val_size/split_train_test_size, \n",
    "                                  stratify=val[colname_stratify], \n",
    "                                  random_state=random_seed)\n",
    "    return train.reset_index(), test, val\n",
    "\n",
    "\n",
    "def sentence_to_nilc_index_token(text, stem=False):\n",
    "    # Traduzindo os tokens do B2W para o index do NILC\n",
    "    tokens = text.lower().split() # Pegar um tokenizer decente...\n",
    "    tokens = [model_w2v.vocab[t].index if t in model_w2v.vocab else 1 for t in tokens ]\n",
    "    return tokens\n",
    "\n",
    "def sort_by_size(df, col_to_sort):\n",
    "    df['sentence_length'] = df[col_to_sort].apply(lambda x: len(x))\n",
    "    df.sort_values(by=['sentence_length'], inplace=True, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def getXY(serieX, serieY, padding_maxlen=50):\n",
    "    x_train = keras.preprocessing.sequence.pad_sequences(train['review_text_clean'], maxlen=padding_maxlen, padding='post')\n",
    "    y_train = train['overall_rating']\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "# ------ main ----\n",
    "df_to_work = b2wCorpus\n",
    "\n",
    "TAMMAX_SENTENCE=50\n",
    "\n",
    "values_to_retain=[1,2,3,4,5]\n",
    "df_to_work = df_to_work[df_to_work['overall_rating'].isin(values_to_retain)]\n",
    "# df_to_work\n",
    "df_to_work['overall_rating'] = df_to_work.overall_rating.apply(lambda x: x-1)\n",
    "\n",
    "# Aplicando o sentence_to_nilc_index_token\n",
    "df_to_work['review_text_clean'] = df_to_work.review_text.apply(lambda x: sentence_to_nilc_index_token(x))\n",
    "\n",
    "# train, test, val = train_test_val_split(df_to_work, train_size=0.75, test_size=0.15)\n",
    "train, test, val = train_test_val_split(df_to_work)\n",
    "\n",
    "sort_by_size(train, 'review_text_clean')\n",
    "\n",
    "\n",
    "x_train, y_train = getXY(train['review_text_clean'], train['overall_rating'], padding_maxlen=TAMMAX_SENTENCE)\n",
    "x_test,  y_test  = getXY(test['review_text_clean'], test['overall_rating'], padding_maxlen=TAMMAX_SENTENCE)\n",
    "x_val,   y_val   = getXY(val['review_text_clean'], val['overall_rating'], padding_maxlen=TAMMAX_SENTENCE)\n",
    "\n",
    "\n",
    "print(\"train..:\", len(train), round(len(train) / len(df_to_work),3) ) \n",
    "print(\"test...:\", len(test), round(len(test) / len(df_to_work),3) )\n",
    "print(\"val....:\", len(val), round(len(val) / len(df_to_work),3) )\n",
    "print(\"--\" * 20) \n",
    "print(\"x_train..:\", len(x_train[-1]), ) \n",
    "print(\"x_test...:\", len(x_test[-1]), ) \n",
    "print(\"x_val....:\", len(x_val[-1]), ) \n",
    "# train = train.reset_index(drop=True)\n",
    "# train = train.reset_index(inplace=True)\n",
    "# train = train.copy()\n",
    "\n",
    "# df_to_work.groupby\n",
    "df_to_work.groupby(['overall_rating']).count()\n",
    "# b2wCorpus.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qnt dados..: 19\n",
      "lote size..: 3\n",
      "lote count..: 7\n",
      "Pegando lote 0 de 7:[0, 1, 2]\n",
      "Pegando lote 1 de 7:[3, 4, 5]\n",
      "Pegando lote 2 de 7:[6, 7, 8]\n",
      "Pegando lote 3 de 7:[9, 10, 11]\n",
      "Pegando lote 4 de 7:[12, 13, 14]\n",
      "Pegando lote 5 de 7:[15, 16, 17]\n",
      "Pegando lote 6 de 7:[18]\n"
     ]
    }
   ],
   "source": [
    "dados = list(range(19))\n",
    "lote_size = 3\n",
    "lote_count = int(np.ceil(len(dados)/ lote_size))\n",
    "print(\"qnt dados..:\", len(dados))\n",
    "print(\"lote size..:\", lote_size)\n",
    "print(\"lote count..:\", lote_count)\n",
    "for i in range(0,lote_count):\n",
    "    print(f\"Pegando lote {i} de {lote_count}:\", end=\"\")\n",
    "    print(dados[ i*lote_size : i*lote_size+lote_size ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            10000000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 10,029,765\n",
      "Trainable params: 10,029,765\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras import Sequential\n",
    "# from keras.utils import Sequence\n",
    "# from keras.layers import LSTM, Dense, Masking\n",
    "# import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "# model = tf.keras.Sequential([\n",
    "from tensorflow import keras\n",
    "\n",
    "def get_lstm_model(dropout_prob=0.0):\n",
    "    embedding_layer = model_w2v.get_keras_embedding()\n",
    "    embedding_layer.trainable = True\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(TAMMAX_SENTENCE, )))\n",
    "    model.add(embedding_layer)\n",
    "    model.add(layers.LSTM(64))\n",
    "    model.add(layers.Dropout(dropout_prob))\n",
    "    model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "    model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = get_lstm_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2482/2482 [==============================] - 502s 202ms/step - loss: 1.0968 - accuracy: 0.5304 - val_loss: 0.9856 - val_accuracy: 0.5816\n",
      "Epoch 2/10\n",
      "2482/2482 [==============================] - 479s 193ms/step - loss: 0.9723 - accuracy: 0.5837 - val_loss: 0.9067 - val_accuracy: 0.6090\n",
      "Epoch 3/10\n",
      "2482/2482 [==============================] - 475s 191ms/step - loss: 0.9144 - accuracy: 0.6074 - val_loss: 0.8563 - val_accuracy: 0.6374\n",
      "Epoch 4/10\n",
      "2482/2482 [==============================] - 497s 200ms/step - loss: 0.8659 - accuracy: 0.6278 - val_loss: 0.7989 - val_accuracy: 0.6608\n",
      "Epoch 5/10\n",
      "2482/2482 [==============================] - 517s 208ms/step - loss: 0.8173 - accuracy: 0.6513 - val_loss: 0.7530 - val_accuracy: 0.6869\n",
      "Epoch 6/10\n",
      "2482/2482 [==============================] - 484s 195ms/step - loss: 0.7687 - accuracy: 0.6753 - val_loss: 0.7032 - val_accuracy: 0.7089\n",
      "Epoch 7/10\n",
      "2482/2482 [==============================] - 479s 193ms/step - loss: 0.7177 - accuracy: 0.6995 - val_loss: 0.6508 - val_accuracy: 0.7330\n",
      "Epoch 8/10\n",
      "2482/2482 [==============================] - 471s 190ms/step - loss: 0.6724 - accuracy: 0.7187 - val_loss: 0.6090 - val_accuracy: 0.7467\n",
      "Epoch 9/10\n",
      "2482/2482 [==============================] - 470s 189ms/step - loss: 0.6283 - accuracy: 0.7366 - val_loss: 0.5672 - val_accuracy: 0.7635\n",
      "Epoch 10/10\n",
      "2482/2482 [==============================] - 467s 188ms/step - loss: 0.5900 - accuracy: 0.7509 - val_loss: 0.5269 - val_accuracy: 0.7812\n",
      "2482/2482 [==============================] - 21s 9ms/step - loss: 0.5269 - accuracy: 0.7812\n",
      "Loss:  0.5268877744674683\n",
      "Accuracy:  0.7812220454216003\n"
     ]
    }
   ],
   "source": [
    "# Ver lista06\n",
    "\n",
    "# Ler aqui pro batch generator:\n",
    "#     https://datascience.stackexchange.com/questions/48796/how-to-feed-lstm-with-different-input-array-sizes\n",
    "\n",
    "# Seu código aqui\n",
    "\n",
    "QNT_EPOCAS_TREINO = 10\n",
    "\n",
    "\n",
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=QNT_EPOCAS_TREINO, validation_data=(x_val, y_val)\n",
    ")\n",
    "\n",
    "loss, accuracy = model.evaluate(x=x_test,y=y_test)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

